# -*- coding: utf-8 -*-
"""Kalbe_Nutritionals_Rakamin_DS_PBI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RRWwbIVMuSNLbevWT9SMzfiFHQxo1Hjv

#Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats import iqr

from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

from sklearn.metrics import mean_squared_error
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from yellowbrick.cluster import SilhouetteVisualizer

!pip install pmdarima
from pmdarima import auto_arima
from pmdarima.utils import tsdisplay

"""#Dataset"""

file1 = 'https://docs.google.com/spreadsheets/d/1hJ1Y33SdfsDT0F6Gju1DYO4vfPh9ROx0Lxe-r60mgUQ/edit#gid=2077295731'
file2 = 'https://docs.google.com/spreadsheets/d/19lBCG3W1AeYObLzzGUGbMkOBiWytHvN9BhUgvIjYmQw/edit#gid=823865895'
file3 = 'https://docs.google.com/spreadsheets/d/1MV5JbTgTy4qhkKQi7n3yV_Wcpf4HhKXSh5gCjHhpNps/edit#gid=544497479'
file4 = 'https://docs.google.com/spreadsheets/d/1xoB9FYmumtqJmGvajBSwbP60xyqjijGorUn4lm4qlRI/edit#gid=1079280484'

file1_trf = file1.replace('/edit#gid=', '/export?format=csv&gid=')
file2_trf = file2.replace('/edit#gid=', '/export?format=csv&gid=')
file3_trf = file3.replace('/edit#gid=', '/export?format=csv&gid=')
file4_trf = file4.replace('/edit#gid=', '/export?format=csv&gid=')

transaction_df = pd.read_csv(file1_trf, parse_dates=['Date'], dayfirst=True)
transaction_df.head()

transaction_df.info()

product_df = pd.read_csv(file2_trf)
product_df

product_df.info()

customer_df = pd.read_csv(file3_trf)
customer_df

customer_df.info()

store_df = pd.read_csv(file4_trf)
store_df

store_df.info()

"""#Data Cleaning

##Handling null
"""

#transaction_df
transaction_df.isna().sum()

# product_df
product_df.isna().sum()

# customer_df - found null in marital
customer_df.isna().sum()

#I don't remove null values because: 1. ARIMA will not use marital status data, 2. Kmeans clustering also will not use marital status
customer_df[customer_df['Marital Status'].isna()]

# store_df
store_df.isna().sum()

"""##Handling data type"""

# transaction_df
transaction_df.info()

# transaction_df - change customerid, productid, storeid to string
type_col = ['CustomerID', 'ProductID', 'StoreID']
for i in type_col:
  transaction_df[i] = transaction_df[i].astype('str')

transaction_df.info()

# product_df
product_df.info()

# product_df - change productid to str
product_df['ProductID'] = product_df['ProductID'].astype('str')
product_df.info()

# customer_df
customer_df.info()

# customer_df - change customerid to str
customer_df['CustomerID'] = customer_df['CustomerID'].astype('str')

# customer_df - assign gender 0 to Wanita, gender 1 to Pria
customer_df['Gender'] = np.where(customer_df['Gender']==0, 'Wanita',
                                 np.where(customer_df['Gender']==1, 'Pria', np.nan))
customer_df['Gender'].unique()

customer_df['Income'].head()

# customer_df - replace , to . in Income and change to float
customer_df['Income'] = customer_df['Income'].str.replace(',','.').astype('float')

customer_df.info()

# store_df
store_df.info()

# store_df - change storeid to str
store_df['StoreID'] = store_df['StoreID'].astype('str')

store_df[['Latitude', 'Longitude']]

store_df.info()

"""##Handling duplicate"""

# transaction_df
transaction_df.info()

# transaction_df - allduplicate
transaction_df.duplicated().value_counts(dropna=False)

# transaction_df - transactionid+productid duplicate - this should be asked to team that responsible in the
# data pipeline, from inputing to arrival at data mart/data warehouse
transaction_df.duplicated(subset=['TransactionID','ProductID']).value_counts()

# product_df
product_df.duplicated().value_counts()

# customer_df
customer_df.info()

# customer_df - allduplicate
customer_df.duplicated().value_counts()

# store_df
store_df.duplicated().value_counts()

"""##Handling illogical data and outlier"""

# transaction_df
transaction_df.info()

# I don't remove outlier in this step, I'll remove when it's used in Kmeans clustering
fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(18,3))

for i,v in enumerate(transaction_df.select_dtypes('int').columns):
  temp = transaction_df[[v]]
  sns.boxplot(data=temp, ax=ax[i], orient='h')

fig.tight_layout()
plt.show()

# product_df
product_df.info()

sns.boxplot(data=product_df[['Price']], orient='h')
plt.show()

# customer_df
customer_df.info()

# I don't remove outlier in this step, I'll remove when it's used in Kmeans clustering
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,3))

for i,v in enumerate(customer_df.select_dtypes(np.number).columns):
  temp = customer_df[[v]]
  sns.boxplot(data=temp, ax=ax[i], orient='h')

fig.tight_layout()
plt.show()

# store_df
store_df.info()
display(store_df)

"""#ARIMA

##Identify Seasonality
"""

# prepare the data
arima_df = transaction_df[['Date', 'Qty']].copy().sort_values(by='Date').groupby('Date')[['Qty']].sum()
arima_df.info()
display(arima_df.head())

# train_test_split
arima_df_len = round(len(arima_df)*0.8)
y_train, y_test = arima_df[['Qty']].iloc[:arima_df_len], arima_df[['Qty']].iloc[arima_df_len:]

fig, ax = plt.subplots(figsize=(20,5))
y_train.plot(ax=ax)
y_test.plot(ax=ax, c='orange')

handles, labels = ax.get_legend_handles_labels()
ax.legend([handles[0],handles[1]],
          ['y_train', 'y_test'],
          loc='upper right', fontsize=10,
          title='', bbox_to_anchor=(0.995, 0.995), borderaxespad=0.0)
ax.set_title('Train Test Split')
plt.show()
fig.savefig('ttsplit.png', format='png', bbox_inches='tight', dpi=200)

#Determine seasonality
di = {}

for i in range(2, 30, 1):
  acf_data = y_train.copy()
  acf_data = acf_data - acf_data.rolling(i).mean()
  acf_data.dropna(inplace=True)

  try:
    result = adfuller(acf_data)
    di[i] = result[0].round(10)
  except:
    break

fig, ax = plt.subplots(figsize=(20,7))
plt.plot(di.keys(), di.values(), marker='o')
for i,v in di.items():
  plt.text(i, v, round(v,2))
plt.show()

# No seasonality
acf_data = y_train.copy()
acf_data = acf_data - acf_data.rolling(9).mean()
acf_data.dropna(inplace=True)

fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20,10))
plot_acf(acf_data, lags=100, zero=False, ax=ax[0])
sns.lineplot(acf_data, ax=ax[1])
plt.show()
fig.savefig('autocorrelation_daily.png', format='png', bbox_inches='tight', dpi=200)

"""No seasonality

##Decide the correct differencing and create the model
"""

# decide the correct differencing and create model
model = auto_arima(y_train, trace=True, suppress_warnings=True, max_p=20, max_d=20,
                   max_q=20)
model.summary()

"""##Plot diagnostics"""

# plot_diagnostics
fig = model.plot_diagnostics(figsize=(10,10))
plt.show()

fig.savefig('plot_diagnostics.png', format='png', bbox_inches='tight', dpi=200)

"""##Prediction"""

# in-sample
in_sample_len = round(len(y_train)*0.8)
fig, ax = plt.subplots(figsize=(20,5))

insample, conf_int = model.predict_in_sample(start=in_sample_len, return_conf_int=True, alpha=0.05)
insample = insample.reset_index()
conf_int = pd.DataFrame(conf_int, columns=[1,2])
insample = pd.concat([insample, conf_int], axis=1).set_index('index')

ax.plot(y_train, c='blue', label='train')
ax.plot(insample[0], c='orange', label='insample_prediction')
ax.fill_between(insample.index, insample[1], insample[2], color='pink', label='95% confidence interval')
ax.set_title('Train Set Prediction')
ax.legend()
plt.show()
fig.savefig('insample_prediction.png', format='png', bbox_inches='tight', dpi=200)

# Print the train set error:
print("RMSE: %.3f" % np.sqrt(mean_squared_error(y_train[in_sample_len:], insample[0])))

# test_set_prediction
fig, ax = plt.subplots(figsize=(20,5))

y_pred, conf_int = model.predict(n_periods = len(y_test), return_conf_int=True, alpha=0.05)
forecast = y_pred.reset_index().rename(columns={'index':'month'})
conf_int = pd.DataFrame(conf_int, columns=[1,2])
forecast = pd.concat([forecast, conf_int], axis=1).set_index('month').rename(columns=dict(zip([0,1,2],['Qty','LowerBound','UpperBound'])))

ax.plot(arima_df['Qty'], c='blue', label='history')
ax.plot(forecast['Qty'], c='orange', label='test_set_prediction')
ax.fill_between(forecast.index, forecast['LowerBound'], forecast['UpperBound'], color='pink', label='95% confidence interval')
ax.set_title('Test Set Prediction')
ax.legend()
plt.show()
fig.savefig('test_set_prediction.png', format='png', bbox_inches='tight', dpi=200)

# Print the test set error:
print("RMSE: %.3f" % np.sqrt(mean_squared_error(y_test, y_pred)))

# out_of_sample
history = arima_df.copy()
fig, ax = plt.subplots(figsize=(20,5))

model_fit = model.fit(history)
y_pred, conf_int = model_fit.predict(n_periods = 30, return_conf_int=True, alpha=0.05)
forecast = y_pred.reset_index().rename(columns={'index':'month'})
conf_int = pd.DataFrame(conf_int, columns=[1,2])
forecast = pd.concat([forecast, conf_int], axis=1).set_index('month').rename(columns=dict(zip([0,1,2],['Qty','LowerBound','UpperBound'])))
history = pd.concat([history, forecast], axis=0)

ax.plot(history['Qty'].loc[:'2022'], c='blue', label='history')
ax.plot(history['Qty'].loc['2023':], c='orange', label='prediction')
ax.fill_between(history.index, history['LowerBound'], history['UpperBound'], color='pink', label='95% confidence interval')
ax.set_title('Out of Sample Prediction')
ax.legend()
plt.show()
fig.savefig('out_of_sample.png', format='png', bbox_inches='tight', dpi=200)

history.tail()

"""#Kmeans Clustering

## Preparation and Removing Outlier
"""

# Merge All Data
master_df = transaction_df.merge(product_df, on='ProductID', how='left')\
                          .merge(customer_df, on='CustomerID', how='left')\
                          .merge(store_df, on='StoreID', how='left')
master_df.drop(columns='Price_y', inplace=True)
master_df.rename(columns={'Price_x':'Price'}, inplace=True)
master_df.info()
display(master_df.head())

# Groupby: customerid, Aggregate: transaction_count, qty_sum, total_amount_sum
group_customer = master_df.groupby('CustomerID').agg({'TransactionID':'count','Qty':'sum','TotalAmount':'sum'})
group_customer.rename(columns=dict(zip(['TransactionID','Qty','TotalAmount'],
                                       ['transaction_count','qty_sum','total_amount_sum'])),
                      inplace=True)
group_customer.info()
display(group_customer.head())

# Visualize boxplot to see outlier
fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(5,5))

for i,v in enumerate(group_customer.columns):
  temp = group_customer[[v]]
  sns.boxplot(data=temp, ax=ax[i], orient='h')

fig.tight_layout()
plt.show()

# Remove outlier
group_customer_no_outlier = group_customer.copy()

# qty_sum
q1_qty_sum = np.quantile(group_customer_no_outlier['qty_sum'],0.25)
q3_qty_sum = np.quantile(group_customer_no_outlier['qty_sum'],0.75)
lb_qty_sum = q1_qty_sum - 1.5*(iqr(group_customer_no_outlier['qty_sum']))
ub_qty_sum = q3_qty_sum + 1.5*(iqr(group_customer_no_outlier['qty_sum']))
cond_qty_sum = (group_customer_no_outlier['qty_sum']>lb_qty_sum) & (group_customer_no_outlier['qty_sum']<ub_qty_sum)
group_customer_no_outlier = group_customer_no_outlier[cond_qty_sum]

# total_amount_sum
q1_amount_sum = np.quantile(group_customer_no_outlier['total_amount_sum'],0.25)
q3_amount_sum = np.quantile(group_customer_no_outlier['total_amount_sum'],0.75)
lb_amount_sum = q1_amount_sum - 1.5*(iqr(group_customer_no_outlier['total_amount_sum']))
ub_amount_sum = q3_amount_sum + 1.5*(iqr(group_customer_no_outlier['total_amount_sum']))
cond_amount_sum = (group_customer_no_outlier['total_amount_sum']>lb_amount_sum) & (group_customer_no_outlier['total_amount_sum']<ub_amount_sum)
group_customer_no_outlier = group_customer_no_outlier[cond_amount_sum]

# transaction_count
q1_transaction_count = np.quantile(group_customer_no_outlier['transaction_count'],0.25)
q3_transaction_count = np.quantile(group_customer_no_outlier['transaction_count'],0.75)
lb_transaction_count = q1_transaction_count - 1.5*(iqr(group_customer_no_outlier['transaction_count']))
ub_transaction_count = q3_transaction_count + 1.5*(iqr(group_customer_no_outlier['transaction_count']))
cond_transaction_count = (group_customer_no_outlier['transaction_count']>lb_transaction_count) & (group_customer_no_outlier['transaction_count']<ub_transaction_count)
group_customer_no_outlier = group_customer_no_outlier[cond_transaction_count]

# Boxplot after removing outlier
fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(5,5))

for i,v in enumerate(group_customer_no_outlier.columns):
  temp = group_customer_no_outlier[[v]]
  sns.boxplot(data=temp, ax=ax[i], orient='h')

fig.tight_layout()
plt.show()

group_customer_no_outlier.info()

"""## Preprocessing"""

preprocessing = group_customer_no_outlier.copy()

# MinMaxScaler
mm = MinMaxScaler()
mm_preprocessing = mm.fit_transform(preprocessing[preprocessing.columns])

# StandardScaler
ss = StandardScaler()
ss_preprocessing = ss.fit_transform(preprocessing[preprocessing.columns])

"""##Deciding N. Cluster"""

kmeans_data = ss_preprocessing.copy()

# Elbow method
distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k,init='k-means++',random_state=10)
    kmeanModel.fit(kmeans_data)  #---------------------Ini yang diganti
    distortions.append(kmeanModel.inertia_)

fig = plt.figure(figsize=(5,5))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('The Elbow Method showing the optimal k')
plt.show()
print(distortions)
fig.savefig('elbow.png', format='png', bbox_inches='tight', dpi=200)

# Silhouette score plot
K = range(2,11)
max_K = max(K)
fig, ax = plt.subplots(int(np.ceil(max_K/2)), 2, figsize = (15,30))

for n_clusters in K:
  kmeansModel = KMeans(n_clusters=n_clusters, init='k-means++', random_state=10)

  q, mod = divmod(n_clusters,2)
  sil = SilhouetteVisualizer(kmeansModel, is_fitted = False, ax = ax[q-1][mod])
  sil.fit(kmeans_data)
  sil.finalize()
  print(f"For k={n_clusters}, the average silhouette score is {sil.silhouette_score_}")

"""Based on Elbow Method and Silhouette Score, I choose 3 cluster

## Interpretation
"""

clustering_df = kmeans_data.copy()
predict_df = group_customer.copy()
interpretation_df = customer_df.set_index('CustomerID')\
                    .merge(group_customer, left_index=True, right_index=True,
                           how='inner')

# standardized predict_df
predict_df[predict_df.columns] = ss.transform(predict_df[predict_df.columns])

# fit on clustering_df, predict and get label from predict_df
cluster_model = KMeans(n_clusters=3, random_state=10, init='k-means++')
cluster_model.fit(clustering_df)
cluster_label = cluster_model.predict(predict_df)

# input the label into interpretation_df
predict_df['cluster'] = cluster_label
interpretation_df = interpretation_df.merge(predict_df['cluster'], left_index=True,
                                            right_index=True, how='inner')\
                                     .reset_index()
interpretation_df.info()

# descriptive stats
interpretation_df.describe()

# descriptive stats for each cluster
cluster_group = interpretation_df.groupby('cluster')[interpretation_df.columns[:-1]].describe().T
cluster_group

# Age in each cluster
age_cluster = interpretation_df.groupby('cluster')['Age'].mean().to_frame()
age_cluster

# gender ratio in each cluster
gender_cluster = interpretation_df.groupby('cluster')['Gender'].value_counts().to_frame()
gender_cluster

# marital status in each cluster
marital_cluster = interpretation_df.groupby('cluster')['Marital Status'].value_counts(dropna=True).to_frame()
marital_cluster

# Income in each cluster
income_cluster = interpretation_df.groupby('cluster')['Income'].mean().to_frame()
income_cluster

# transaction_count in each cluster
transaction_count_cluster = interpretation_df.groupby('cluster')['transaction_count'].mean().to_frame()
transaction_count_cluster

# qty_sum in each cluster
qty_sum_cluster = interpretation_df.groupby('cluster')['qty_sum'].mean().to_frame()
qty_sum_cluster

# total_amount_sum in each cluster
total_amount_sum_cluster = interpretation_df.groupby('cluster')['total_amount_sum'].mean().to_frame()
total_amount_sum_cluster

"""#Thank you!"""